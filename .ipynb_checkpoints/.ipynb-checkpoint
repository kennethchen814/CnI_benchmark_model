{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normallize the rating migration matrix for each segment so that each row adds up to 1\n",
    "def rel_mat(seg, TM):\n",
    "    rel_mat_t_list = []\n",
    "    for s in range(len(seg)):\n",
    "        rel_mat_t = TM[seg[s]].drop([\"rating_alph\"], axis=1)\n",
    "        for j in range(rel_mat_t.shape[1]):\n",
    "            rel_mat_t.iloc[:,j] = rel_mat_t.iloc[:,j]/rel_mat_t.sum(axis=1)\n",
    "        rel_mat_t_list.append(rel_mat_t)\n",
    "    return rel_mat_t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the MEVs used in regression and generate lags up to 2 quarters\n",
    "def MacroTrans(macro):\n",
    "    period_len = macro.shape[0]\n",
    "    macro[\"period\"] = pd.date_range('1976-03-01', periods=period_len, freq='Q')\n",
    "    \n",
    "    macro = macro[macro.period > '1989-12-31']\n",
    "\n",
    "    macro = macro[[\"period\", \"Real GDP growth\", \"Unemployment rate\", \"10-year Treasury yield\", \"BBB corporate yield\", \n",
    "                   \"Dow Jones Total Stock Market Index (Level)\", \"Market Volatility Index (Level)\", \"WTI Price\",\n",
    "                   \"Euro area real GDP growth\"]]\n",
    "    macro.columns = [\"period\", \"usgdp\", \"ur\", \"tenyr_yield\", \"bbb_yield\", \"djix\", \"vix\", \"wti\", \"eugdp\"]\n",
    "    macro[\"spread\"] = macro[\"bbb_yield\"] - macro[\"tenyr_yield\"]\n",
    "    macro[\"djix_lag4\"] = macro[\"djix\"].shift(periods=4)\n",
    "    macro[\"djgr\"] = macro[\"djix\"]/macro[\"djix_lag4\"] - 1\n",
    "    macro[\"vix_lag4\"] = macro[\"vix\"].shift(periods=4)\n",
    "    macro[\"vixgr\"] = macro[\"vix\"]/macro[\"vix_lag4\"] - 1\n",
    "    macro[\"wti_lag4\"] = macro[\"wti\"].shift(periods=4)\n",
    "    macro[\"wtigr\"] = macro[\"wti\"]/macro[\"wti_lag4\"] - 1\n",
    "    \n",
    "\n",
    "    macro = macro[[\"period\", \"usgdp\", \"ur\", \"spread\", \"djgr\", \"vixgr\", \"wtigr\", \"eugdp\"]]\n",
    "    colname = macro.columns.values\n",
    "    for i in range(1,len(colname)):\n",
    "        macro[colname[i]+\"_lag1\"] = macro[colname[i]].shift(periods=1)\n",
    "        macro[colname[i]+\"_lag2\"] = macro[colname[i]].shift(periods=2)\n",
    "        macro.rename(columns={colname[i]: colname[i]+\"_lag0\"}, inplace=True)\n",
    "\n",
    "    macro.dropna(inplace=True)\n",
    "    return macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the shift factors for all segments in the forecast horizon\n",
    "def shift_list(DR, macro_hist, macro_scenario):\n",
    "    # Generate segment level default rate data\n",
    "    segment2 = [\"Fin_US\",\"Oil_US\",\"Fin_NUS\",\"Oil_NUS\",\"Cyc_NUS\",\"NCy_NUS\",\"Cyc_US1\",\"Cyc_US2\",\"NCy_US1\",\"NCy_US2\",\"Sov\"]\n",
    "    PDlist = {}\n",
    "    for s in range(len(segment2)-5):\n",
    "        PDlist[segment2[s]] = DR[segment2[s]][[\"period\",\"PD1\"]].copy()\n",
    "\n",
    "    # For two segments (US Cyclical and US Non-Cyclical), the default rate is further separated by rating groups\n",
    "    PDlist[\"Cyc_US1\"] = DR[\"Cyc_US_2clst\"][[\"period\",\"PD2\"]].copy()\n",
    "    PDlist[\"Cyc_US2\"] = DR[\"Cyc_US_2clst\"][[\"period\",\"PD3\"]].copy()\n",
    "    PDlist[\"NCy_US1\"] = DR[\"Cyc_US_2clst\"][[\"period\",\"PD2\"]].copy()\n",
    "    PDlist[\"NCy_US2\"] = DR[\"NCy_US\"][[\"period\",\"PD4\"]].copy()\n",
    "    PDlist[\"Sov\"] = DR[\"Sov\"][[\"period\",\"PD1\"]].copy()\n",
    "\n",
    "    # Define MEVs used in each of the segments\n",
    "    MEVlist = {}\n",
    "    MEVlist[\"Fin_US\"] = [\"usgdp_lag0\", \"spread_lag1\"]\n",
    "    MEVlist[\"Oil_US\"] = [\"vixgr_lag0\", \"wtigr_lag1\"]\n",
    "    MEVlist[\"Oil_NUS\"] = [\"spread_lag0\", \"wtigr_lag2\"]\n",
    "    MEVlist[\"Fin_NUS\"] = [\"ur_lag2\", \"spread_lag0\"]\n",
    "    MEVlist[\"Cyc_NUS\"] = [\"spread_lag1\", \"djgr_lag0\"]\n",
    "    MEVlist[\"NCy_NUS\"] = [\"djgr_lag2\", \"vixgr_lag0\"]\n",
    "    MEVlist[\"Cyc_US1\"] = [\"usgdp_lag0\", \"djgr_lag0\"]\n",
    "    MEVlist[\"Cyc_US2\"] = [\"usgdp_lag0\", \"spread_lag1\"]\n",
    "    MEVlist[\"NCy_US1\"] = [\"usgdp_lag0\", \"djgr_lag0\"]\n",
    "    MEVlist[\"NCy_US2\"] = [\"spread_lag2\", \"vixgr_lag0\"]\n",
    "\n",
    "    # Run the regression model of historical default rate on MEVs and generate PD forecast to derive the shift factor\n",
    "    Shiftlist = {}\n",
    "    for s in range(len(segment2)-1):\n",
    "        # Prepare data for regression by merging the MEVs with historical default rate\n",
    "        PDlist[segment2[s]][\"period\"] = pd.date_range(PDlist[segment2[s]][\"period\"][0], periods=PDlist[segment2[s]].shape[0], freq='Q')\n",
    "        PDlist[segment2[s]].rename(columns={PDlist[segment2[s]].columns[1]: \"PD\"}, inplace=True)\n",
    "        RegData_final = PDlist[segment2[s]].merge(macro_hist, left_on=\"period\", right_on=\"period\")\n",
    "        RegData_final.dropna(inplace=True)\n",
    "\n",
    "        # Run fractional logit regression\n",
    "        import statsmodels.api as sm\n",
    "        Y = RegData_final[\"PD\"]\n",
    "        X = sm.add_constant(RegData_final[MEVlist[segment2[s]]])\n",
    "        mod = sm.Logit(Y,X)\n",
    "        res = mod.fit()\n",
    "        #print(res.summary())\n",
    "\n",
    "        # Generate PD forecast\n",
    "        predict = res.predict(sm.add_constant(macro_scenario[MEVlist[segment2[s]]]))\n",
    "        PD_Forecast_Qtrs = pd.concat([macro_scenario[\"period\"].loc[0:12], predict.loc[0:12]], axis=1)\n",
    "        PD_Forecast_Qtrs.columns = [\"period\", \"PD_fcst\"]\n",
    "\n",
    "        # Calculate the shift factor\n",
    "        PD_hist_mean = RegData_final.PD.mean()\n",
    "\n",
    "        PD_Shift = deepcopy(PD_Forecast_Qtrs)\n",
    "        PD_Shift[\"PD_mean\"] = PD_hist_mean\n",
    "        PD_Shift[\"fcst\"] = PD_Shift[\"PD_mean\"].apply(norm.ppf) - PD_Shift[\"PD_fcst\"].apply(norm.ppf)\n",
    "\n",
    "        Shiftlist[segment2[s]] = PD_Shift\n",
    "\n",
    "    # Temporarily use the shift factor of the NCy_US2 segment for the Sovereign segment\n",
    "    # Later need to apply the P-T LDPD method for the Sovereign segment\n",
    "    Shiftlist[\"Sov\"] = PD_Shift\n",
    "    \n",
    "    return Shiftlist, PD_Forecast_Qtrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasted PIT matrices by applying the shift factor on the TTC matrices\n",
    "def pit_mat(pd_fcst, ttc_mat, states, seg, shift):\n",
    "    #--Obtain PIT Transition Matrices from TTC Matrices by Applying Shift Factors to the Adjusted TTC Matrix--#\n",
    "    PIT_mat_fcst = {}\n",
    "    PIT_mat_fcst_AllSeg = {}\n",
    "\n",
    "    for h in range(len(pd_fcst[\"period\"])):\n",
    "        PIT_mat_fcst_AllSeg[h] = np.zeros((states * len(seg), states), dtype = float)\n",
    "\n",
    "    # Define the function to convert TTC matrices to PIT matrices\n",
    "    def PIT_matrix(TTC_mat, shift_table1, shift_table2, cut, quarter):\n",
    "        PIT_mat = deepcopy(TTC_mat)\n",
    "        for i in range(states-1):\n",
    "            shift_table = deepcopy(shift_table1)\n",
    "            if i>cut:\n",
    "                shift_table = deepcopy(shift_table2)\n",
    "            #Last Column of Transition Matrix (i.e. PD) Decreased by the Shift Factor#\n",
    "            PIT_mat.iloc[[i], [states-1]] = norm.cdf(norm.ppf(TTC_mat.iloc[[i],[states-1]]) - shift_table[\"fcst\"][quarter])\n",
    "\n",
    "            #Adjusting Remaining Columns of Matrix up to the 2nd Column#\n",
    "            for m in range(2, states):\n",
    "                start_temp = states - m\n",
    "                if abs(TTC_mat.iloc[[i], start_temp:states].sum(axis=1)-1).values<0.000000001:\n",
    "                    PIT_mat.iloc[[i], start_temp] = 1-PIT_mat.iloc[[i], start_temp+1:states].sum(axis=1)\n",
    "                else:\n",
    "                    PIT_mat.iloc[[i], start_temp] = \\\n",
    "                    norm.cdf(norm.ppf(TTC_mat.iloc[[i], start_temp:states].sum(axis=1)) - shift_table[\"fcst\"][quarter]) \\\n",
    "                    - norm.cdf(norm.ppf(TTC_mat.iloc[[i], start_temp+1:states].sum(axis=1)) - shift_table[\"fcst\"][quarter])\n",
    "\n",
    "            #First Column of Matrix Defined to Ensure Summation of Row to 1#\n",
    "            PIT_mat.iloc[[i], 0] = 1 - PIT_mat.iloc[[i], 1:states].sum(axis=1)\n",
    "\n",
    "        return PIT_mat\n",
    "\n",
    "    #Call Function each segment and Store PIT Matrices in Lists#\n",
    "    for s in range(len(seg)):\n",
    "        print(seg[s])\n",
    "        PIT_mat_fcst_list = []\n",
    "        for h in range(len(pd_fcst[\"period\"])):\n",
    "            if seg[s] in [\"Fin_US\", \"Fin_NUS\", \"Oil_US\", \"Oil_NUS\", \"Cyc_NUS\", \"NCy_NUS\", \"Sov\"]:      \n",
    "                PIT_mat_fcst_list.append(PIT_matrix(TTC_mat = ttc_mat[s], shift_table1 = deepcopy(shift[seg[s]]), \\\n",
    "                                                  shift_table2 = deepcopy(shift[seg[s]]), cut=21, quarter=h))\n",
    "            elif seg[s] == \"Cyc_US_2clst\":\n",
    "                PIT_mat_fcst_list.append(PIT_matrix(TTC_mat = ttc_mat[s], shift_table1 = deepcopy(shift[\"Cyc_US1\"]), \\\n",
    "                                                  shift_table2 = deepcopy(shift[\"Cyc_US2\"]), cut=18, quarter=h))\n",
    "            else:\n",
    "                PIT_mat_fcst_list.append(PIT_matrix(TTC_mat = ttc_mat[s], shift_table1 = deepcopy(shift[\"NCy_US1\"]), \\\n",
    "                                                  shift_table2 = deepcopy(shift[\"NCy_US2\"]), cut=18, quarter=h))\n",
    "            PIT_mat_fcst_AllSeg[h][s*states:(s+1)*states,:] = PIT_mat_fcst_list[h]\n",
    "        PIT_mat_fcst[s] = PIT_mat_fcst_list\n",
    "    \n",
    "    return PIT_mat_fcst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the marginal PD forecast using the forecasted PIT transition matrices\n",
    "def mpd_fcst(pit_mat, ttc_mat, states, horizon, seg, rs_len, reversion):\n",
    "    #--Store Marginal PD from PIT Matrices in One Table and Power Cumulative Matrix till Maximum Horizon --#\n",
    "\n",
    "    PIT_pd_fcst_AllSeg = np.zeros((states*len(seg), horizon), dtype=float)\n",
    "    for s in range(len(seg)):\n",
    "        #Cumulative Transition Matrix Till Period within reasonable and supportable period and Storing Marginal PDs#\n",
    "        PIT_pd_fcst = np.zeros((states, horizon), dtype=float)\n",
    "        PIT_Cumulative_fcst = pit_mat[s][0].values\n",
    "        PIT_pd_fcst[:, 0] = PIT_Cumulative_fcst[:,states-1]\n",
    "\n",
    "        for h in range(1, rs_len):\n",
    "            mat_cum_temp_prev = PIT_Cumulative_fcst.copy()\n",
    "            PIT_Cumulative_fcst = np.dot(PIT_Cumulative_fcst, pit_mat[s][h])\n",
    "            PIT_pd_fcst[:, h] = PIT_Cumulative_fcst[:, states-1] - mat_cum_temp_prev[:,states-1]\n",
    "\n",
    "        #Use Cumulative TM and Multiply it Further to Obtain Marginal PDs in Future Horizons  \n",
    "        def Marg_PD(cum_mat, last_mat, margPD_table):\n",
    "            mat_cum_temp = cum_mat.copy()\n",
    "            for h in range(rs_len, horizon):\n",
    "                mat_cum_temp_prev = deepcopy(mat_cum_temp)\n",
    "                mat_cum_temp = np.dot(mat_cum_temp, last_mat)\n",
    "                margPD_table[:, h] = mat_cum_temp[:, states-1] - mat_cum_temp_prev[:, states-1]\n",
    "            return margPD_table\n",
    "\n",
    "        # Two options for the transition matrix used during the mean reversion period\n",
    "        # Option1: Use TTC matrix\n",
    "        # Option2: Use last PIT matrix\n",
    "        if reversion == 1:\n",
    "            reversion_mat = ttc_mat[s]\n",
    "        else:\n",
    "            reversion_mat = PIT_mat_fcst[s][rs_len-1]\n",
    "\n",
    "        PIT_pd_fcst_f = Marg_PD(cum_mat = deepcopy(PIT_Cumulative_fcst), last_mat = deepcopy(reversion_mat), margPD_table = deepcopy(PIT_pd_fcst))\n",
    "        PIT_pd_fcst_AllSeg[s*states:(s+1)*states, :] = PIT_pd_fcst_f\n",
    "\n",
    "    return PIT_pd_fcst_AllSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ECL forecast using the predicted marginal PD and forecast of other risk parameters\n",
    "def ecl_fcst(data, mpd, states, horizon, seg, rs_len, LGD_option, PP_option, PP_val):\n",
    "\n",
    "    # Calculate provisions\n",
    "    Data_temp1 = deepcopy(data)\n",
    "\n",
    "    #--Define Columns Necessary for CECL Calculation --#\n",
    "    #CECL: Expected Maturity#\n",
    "    Data_temp1[\"CECL_RemMat\"] =  Data_temp1[\"RemMat\"]\n",
    "    #CECL: Exposure#\n",
    "    Data_temp1[\"Exp\"] = Data_temp1[\"EAD\"]\n",
    "\n",
    "    #--Initial Number of Columns + 1--#\n",
    "    col_initial = Data_temp1.shape[1] + 1\n",
    "\n",
    "    #--Populate with Empty CECL Coloumns till Maximum Horizon --#\n",
    "    for i in range(1,horizon):\n",
    "        Data_temp1[\"CECL_ECL_\"+str(i)] = 0\n",
    "\n",
    "\n",
    "    #--a) Create Maginal PD until Maximum Horizon --#\n",
    "    Data_MPD = deepcopy(Data_temp1)\n",
    "    Data_MPD[\"obs\"] = Data_MPD.index+1\n",
    "    Data_MPD = Data_MPD[[\"Rating\", \"obs\", \"Segment\"]]\n",
    "\n",
    "    MPD = pd.DataFrame(mpd)\n",
    "    for i in range(0, horizon):\n",
    "        MPD.rename(columns={i: \"MPD_\"+str(i+1)}, inplace=True)\n",
    "\n",
    "    MPD[\"Rating\"] = np.tile(np.arange(1,states+1), len(seg))\n",
    "    MPD[\"Segment\"] = np.repeat(seg, states)\n",
    "\n",
    "    Data_MPD = pd.merge(Data_MPD, MPD, how=\"left\", on=[\"Rating\", \"Segment\"])\n",
    "\n",
    "    #--b) Create Prepayment Adjusted Marginal PD till Maximum Horizon --#\n",
    "    Data_AdjMPD = deepcopy(Data_temp1)\n",
    "    Data_AdjMPD[\"obs\"] = Data_AdjMPD.index+1\n",
    "    Data_AdjMPD = Data_AdjMPD[[\"Rating\", \"obs\", \"Prepayment\", \"CECL_RemMat\", \"Segment\", \"Fee_Ind\", \"FAS114_ind\"]]\n",
    "\n",
    "    #--Set prepayment based on prepayment rate from data or user specified prepayment rate--#\n",
    "    if PP_option == 1:\n",
    "        Data_AdjMPD[\"pp\"] = PP_val\n",
    "    else:\n",
    "        Data_AdjMPD[\"pp\"] = Data_AdjMPD[\"Prepayment\"]\n",
    "\n",
    "    for i in range(0, horizon):\n",
    "        Data_AdjMPD[\"adj_mpd_\"+str(i+1)] = 0\n",
    "\n",
    "\n",
    "    cum_pd1_prev = Data_MPD[\"MPD_1\"]\n",
    "    cond_pd_prev = Data_MPD[\"MPD_1\"]\n",
    "    cond_pp_prev = Data_AdjMPD[\"pp\"]\n",
    "    sur_prev     = np.repeat(1, len(cond_pp_prev))\n",
    "    adj_mpp_prev = Data_AdjMPD[\"pp\"]\n",
    "    cum_pd2_prev = Data_MPD[\"MPD_1\"]\n",
    "    cum_pp_prev  = Data_AdjMPD[\"pp\"]\n",
    "\n",
    "    Data_AdjMPD[\"adj_mpd_1\"] = Data_MPD[\"MPD_1\"]\n",
    "    for i in range(1, horizon):\n",
    "        cum_pd1_curr = cum_pd1_prev + Data_MPD[\"MPD_\"+str(i+1)] # current period cum PD based on unadjusted PD\n",
    "        cond_pd_curr = Data_MPD[\"MPD_\"+str(i+1)] / (1 - cum_pd1_prev) # current period conditional PD\n",
    "        cond_pp_curr = deepcopy(cond_pp_prev) # current period conditional prepayment rate\n",
    "        sur_curr     = 1 - cum_pd2_prev - cum_pp_prev # current period surviorship\n",
    "        Data_AdjMPD[\"adj_mpd_\"+str(i+1)] = cond_pd_curr * sur_curr # current period unconditional PD adjusted for prepay and survive\n",
    "        adj_mpp_curr = cond_pp_curr * sur_curr # current period prepayment rate adjusted for default and survive\n",
    "        cum_pd2_curr = cum_pd2_prev + Data_AdjMPD[\"adj_mpd_\"+str(i+1)] # current period cum PD based on adjusted unconditional PD\n",
    "        cum_pp_curr  = cum_pp_prev + adj_mpp_prev # current period cumulative prepayment rate based on adjusted prepayment rate\n",
    "        # Set all previous period values to current period values for next period calculation\n",
    "        cum_pd1_prev = deepcopy(cum_pd1_curr)\n",
    "        cond_pd_prev = deepcopy(cond_pd_curr)\n",
    "        cond_pp_prev = deepcopy(cond_pp_curr)\n",
    "        sur_prev     = deepcopy(sur_curr)\n",
    "        adj_mpp_prev = deepcopy(adj_mpp_curr)\n",
    "        cum_pd2_prev = deepcopy(cum_pd2_curr)\n",
    "        cum_pp_prev  = deepcopy(cum_pp_curr)\n",
    "\n",
    "    #For certain outstanding (deferred fees, uneanred discount, etc), use marginal PD without prepayment impact\n",
    "    for i in range(1, horizon+1):\n",
    "        Data_AdjMPD.loc[Data_AdjMPD.Fee_Ind==1, \"adj_mpd_\"+str(i)] = Data_MPD.loc[Data_AdjMPD.Fee_Ind==1, \"MPD_\"+str(i)]\n",
    "\n",
    "    #Set marginal PD of the impaired loans (FAS 114) to 1 in the first quarter and to 0 beyond the first quarter #\n",
    "    Data_AdjMPD.loc[Data_AdjMPD.FAS114_ind==1, \"adj_mpd_1\"] = 1\n",
    "    for i in range(2, horizon+1):\n",
    "        Data_AdjMPD.loc[Data_AdjMPD.FAS114_ind==1, \"adj_mpd_\"+str(i)] = 0\n",
    "\n",
    "    #--c) Create LGD till Maximum Horizon --#\n",
    "\n",
    "    Data_LGD = deepcopy(Data_temp1)\n",
    "    Data_LGD[\"obs\"] = Data_LGD.index+1\n",
    "    Data_LGD = Data_LGD[[\"Rating\", \"obs\", \"LGD\", \"CECL_RemMat\", \"Segment\"]]\n",
    "\n",
    "    if LGD_option == 1:\n",
    "        for i in range(1, rs_len+1):\n",
    "            Data_LGD[\"lgd_\"+str(i)] = LGD_val\n",
    "    else:\n",
    "        Data_LGD[\"FJ_K\"] = (norm.ppf(Data_AdjMPD[\"adj_mpd_1\"]) - norm.ppf(Data_AdjMPD[\"adj_mpd_1\"]*Data_LGD[\"LGD\"]))/(1 - 0.24)**0.5\n",
    "        for i in range(1, rs_len+1):\n",
    "            Data_LGD[\"lgd_\"+str(i)] = norm.cdf(norm.ppf(Data_AdjMPD[\"adj_mpd_\"+str(i)]) - Data_LGD[\"FJ_K\"])/Data_AdjMPD[\"adj_mpd_\"+str(i)]\n",
    "\n",
    "    for i in range(rs_len+1, horizon+1):\n",
    "        Data_LGD[\"lgd_\"+str(i)] = 0.5\n",
    "\n",
    "    #--d) Create EAD till Maximum Horizon --#\n",
    "    Data_EAD = deepcopy(Data_temp1)\n",
    "    Data_EAD[\"obs\"] = Data_EAD.index+1\n",
    "    Data_EAD = Data_EAD[[\"Rating\", \"obs\", \"Exp\", \"CECL_RemMat\", \"Segment\", \"Amort_Type\", \"EIR\"]]\n",
    "\n",
    "    for i in range(0, horizon):\n",
    "        Data_EAD[\"ead_\"+str(i+1)] = 0\n",
    "\n",
    "    #--Set EAD based on Amortization Type--#\n",
    "    cond1 = (Data_EAD.Amort_Type==\"IO\") | (Data_EAD.Amort_Type==\"Irregular\") | ((Data_EAD.Amort_Type==\"Level\") & (Data_EAD.EIR==0))\n",
    "    cond2 = Data_EAD.Amort_Type==\"StraightLine\"\n",
    "    cond3 = (Data_EAD.Amort_Type==\"Level\") & (Data_EAD.EIR>0)\n",
    "    for i in range(1, horizon+1):\n",
    "        Data_EAD.loc[cond1, \"ead_\"+str(i)] = Data_EAD.loc[cond1, \"Exp\"]\n",
    "        Data_EAD.loc[cond2, \"ead_\"+str(i)] = Data_EAD.loc[cond2, \"Exp\"]*(1 - ((i-1)/Data_EAD[\"CECL_RemMat\"]))\n",
    "        Data_EAD.loc[cond3, \"ead_\"+str(i)] = Data_EAD.loc[cond3, \"Exp\"]* \\\n",
    "        ((1+Data_EAD.loc[cond3, \"EIR\"]/4)**Data_EAD[\"CECL_RemMat\"] - (1+Data_EAD.loc[cond3, \"EIR\"]/4)**(i-1)) / ((1+Data_EAD.loc[cond3, \"EIR\"]/4)**Data_EAD[\"CECL_RemMat\"] - 1)\n",
    "\n",
    "    #--e) Calculate ECL till Maximum Horizon --#\n",
    "    for i in range(1, horizon+1):\n",
    "        cond1 = Data_temp1.CECL_RemMat < i-1\n",
    "        cond2 = (Data_temp1.CECL_RemMat >= i-1) & (Data_temp1.CECL_RemMat < i)\n",
    "        cond3 = Data_temp1.CECL_RemMat >= i\n",
    "\n",
    "        Data_temp1.loc[cond1, \"CECL_ECL_\"+str(i)] = 0\n",
    "        Data_temp1.loc[cond2, \"CECL_ECL_\"+str(i)] = Data_AdjMPD.loc[cond2, \"adj_mpd_\"+str(i)] \\\n",
    "        * Data_LGD.loc[cond2, \"lgd_\"+str(i)] * Data_EAD.loc[cond2, \"ead_\"+str(i)] * (Data_temp1.loc[cond2, \"CECL_RemMat\"] - (i-1))\n",
    "        Data_temp1.loc[cond3, \"CECL_ECL_\"+str(i)] = Data_AdjMPD.loc[cond3, \"adj_mpd_\"+str(i)] \\\n",
    "        * Data_LGD.loc[cond3, \"lgd_\"+str(i)] * Data_EAD.loc[cond3, \"ead_\"+str(i)]\n",
    "\n",
    "    col_final = Data_temp1.shape[1]\n",
    "    Data_temp1[\"ECL_Total\"] = Data_temp1.iloc[:,col_initial-1:col_final].sum(axis=1)\n",
    "    \n",
    "    return Data_temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecl_plot(data, horizon):\n",
    "    ECL_qtr = data[data.columns[-(1+horizon):-1]].sum(axis=0)\n",
    "    yr = math.ceil(horizon/4)\n",
    "    ECL_yr = np.zeros(yr+1)\n",
    "    for i in range(0, yr):\n",
    "        ECL_yr[i] = ECL_qtr[i*4:(i+1)*4].sum()/1000000\n",
    "    ECL_yr[yr] = ECL_yr[0:yr].sum()\n",
    "\n",
    "    ECL_plot = pd.DataFrame(ECL_yr)\n",
    "    new_index = []\n",
    "    for i in range(1, yr+1):\n",
    "        new_index.append('Yr_'+str(i))\n",
    "    new_index.append('Total') \n",
    "    ECL_plot['new_index'] = new_index\n",
    "    ECL_plot.set_index('new_index', inplace=True)\n",
    "\n",
    "    trace = go.Bar(\n",
    "        x = ECL_plot.index, \n",
    "        y = ECL_plot[ECL_plot.columns[0]],\n",
    "        text = [round(x, 2) for x in ECL_plot[ECL_plot.columns[0]].tolist()],\n",
    "        textposition='auto',\n",
    "        marker=dict(\n",
    "            color=['rgba(0, 157, 255, 1)', 'rgba(0, 157, 255, 1)',\n",
    "                   'rgba(0, 157, 255, 1)', 'rgba(0, 157, 255, 1)',\n",
    "                   'rgba(0, 157, 255, 1)', 'rgba(0, 157, 255, 1)',\n",
    "                   'rgba(255, 234, 5, 1)']),\n",
    "    )\n",
    "\n",
    "    data = [trace]\n",
    "    layout = go.Layout(\n",
    "        title = 'Annual ECL in $million',\n",
    "        yaxis = dict(\n",
    "        title = 'ECL')\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout = layout)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.011133\n",
      "         Iterations 9\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.030832\n",
      "         Iterations 9\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.006753\n",
      "         Iterations 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.044815\n",
      "         Iterations 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.022509\n",
      "         Iterations 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.029479\n",
      "         Iterations 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.038066\n",
      "         Iterations 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.566189\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.038066\n",
      "         Iterations 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.651920\n",
      "         Iterations 5\n",
      "Fin_US\n",
      "Oil_US\n",
      "Cyc_US_2clst\n",
      "NCy_US\n",
      "Fin_NUS\n",
      "Oil_NUS\n",
      "Cyc_NUS\n",
      "NCy_NUS\n",
      "Sov\n",
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~tjt191/0 or inside your plot.ly account where it is named 'basic-bar'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~tjt191/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ECL_Calc(reversion = 1, scenario = 1, rs_len = 12, LGD_option = 2, PP_option = 2, PP_val = 0.01):\n",
    "    \n",
    "    account_data = pd.read_csv(\"Jumbo_g.csv\")\n",
    "\n",
    "    # Import segment level TTC transition matrix and historical default rate\n",
    "    segment = [\"Fin_US\",\"Oil_US\",\"Cyc_US_2clst\",\"NCy_US\",\"Fin_NUS\",\"Oil_NUS\",\"Cyc_NUS\",\"NCy_NUS\",\"Sov\"]\n",
    "    TMlist = {}\n",
    "    DRlist = {}\n",
    "\n",
    "    for s in segment:\n",
    "        TMlist[s] = pd.read_excel(\"TM_AllSeg.xls\", sheet_name = s)\n",
    "        DRlist[s] = pd.read_csv(\"DR_\"+s+\".csv\")\n",
    "        \n",
    "    # Generate the number of ratings and number of forecast period based on TTC transition matrix \n",
    "    # and the maximum remaining maturity of the portfolio\n",
    "    ttc_mat = rel_mat(segment, TMlist) # Normallize the rating migration matrix for each segment so that each row adds up to 1\n",
    "    states = ttc_mat[0].shape[1]\n",
    "    horizon = max(20, math.ceil(account_data[\"RemMat\"].max() + 0.0001))\n",
    "    \n",
    "    # Import historical macroeconomic variables and scenario forecast\n",
    "    macro_hist = pd.read_csv(\"Supervisory historical Domestic.csv\")\n",
    "    macro_base = pd.read_csv(\"base.csv\")\n",
    "    macro_adv = pd.read_csv(\"adv.csv\")\n",
    "    macro_sa = pd.read_csv(\"sa.csv\")\n",
    "\n",
    "    if scenario == 'ba':\n",
    "        macro_scenario = macro_hist.append(macro_base)\n",
    "    elif scenario == 'ad':\n",
    "        macro_scenario = macro_hist.append(macro_adv)\n",
    "    else:\n",
    "        macro_scenario = macro_hist.append(macro_sa)\n",
    "    \n",
    "    # Keep the MEVs used in regression and generate lags up to 2 quarters\n",
    "    macro_hist_final = MacroTrans(macro_hist)\n",
    "    macro_scenario_final = MacroTrans(macro_scenario)\n",
    "    \n",
    "    # Generate the shift factors for all segments in the forecast horizon\n",
    "    Shiftlist, PD_Forecast_Qtrs = shift_list(DRlist, macro_hist_final, macro_scenario_final)\n",
    "    \n",
    "    # Generate forecasted PIT matrices by applying the shift factor on the TTC matrices\n",
    "    PIT_mat_fcst = pit_mat(PD_Forecast_Qtrs, ttc_mat, states, segment, Shiftlist)\n",
    "    \n",
    "    # Generate the marginal PD forecast using the forecasted PIT transition matrices\n",
    "    Marginal_PD = mpd_fcst(PIT_mat_fcst, ttc_mat, states, horizon, segment, rs_len, reversion)\n",
    "    \n",
    "    # Generate ECL forecast using the predicted marginal PD and forecast of other risk parameters\n",
    "    ECL_loan = ecl_fcst(account_data, Marginal_PD, states, horizon, segment, rs_len, LGD_option, PP_option, PP_val)\n",
    "    \n",
    "    # Generate figure for ECL forecast\n",
    "    fig = ecl_plot(ECL_loan, horizon)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
